# Talking points for Point 1:

## It's evolved really quickly
> Have you seen AI when DALL-E was first unveiled? Back then, everything
> generated by AI was very easy to detect. Over time, however, AI has learned
> to draw hands properly, emulate camera imperfections, and even fabricate
> human mistakes.

## It's trained on everyone's data
> The reason it's learned so quickly wasn't just because of advancements made
> by NVIDIA, but moreso the mass scraping of everyone's data from the internet.
> Therefore, any output of AI is an amalgamation of all the hard work of every
> artist, writer, and intellectual whose work is publicly available on the web.
> All this, without anyone's permission nor even awareness that this was
> occurring.

# Talking points for Point 2:

# Talking points for Point 3:

# Talking points for Point 4:

## For website owners: Anti-AI scraping measures (saves hosting costs)
> Because AI scrapers artificially drive up traffic (and therefore site costs),
> websites can employ filters and other measures to prevent AI scrapers from
> driving up the costs of hosting a site.

## For the education sector: Documents with History Views
> "AI Detection" is terrible and must NOT be used. Instead, look for suspicious
> behaviour as a student writes; if they copy-paste large swathes of text or
> pauses for a suspiciously long time. To detect these behaviours, instruct
> your students to use a service like Google Docs that allows you to inspect a
> document's history.

## For artists: Watermarks and Digital Poisons
> Poisons are so far one of the only few methods to prevent an AI scraper from
> accurately classifying your work, but even then poisons can be very quickly
> counteracted and nullified by AI companies. To supplement this, you may also
> add watermarks; one very obvious one in the corner and a few very subtle ones
> throughout your art. That way, you can at least prove you own your art.
